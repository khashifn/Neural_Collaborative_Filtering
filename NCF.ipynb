{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "colab_type": "code",
        "id": "kDqH1fKaUUF8",
        "outputId": "53c789b1-510d-4b40-db91-7a7a61eafbda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#to use it in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "WQRDBAJVUZ-T",
        "outputId": "f362aab5-cbbe-4c52-bf16-a94452f9dfcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow import keras\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KPdRYufGUbg-"
      },
      "outputs": [],
      "source": [
        "#Loading Dataset and removing not needed parts\n",
        "#loading dataset parameters for model\n",
        "\n",
        "#ratings = pd.read_csv('/content/drive/My Drive/NCF1/1m/ratings.dat',sep='::',names=['UserID','MovieID','Rating','Timestamp'])\n",
        "ratings = pd.read_csv('/content/drive/My Drive/NCF1/100k/u.data',sep='\\t',names=['UserID','MovieID','Rating','Timestamp'])\n",
        "\n",
        "ratings=ratings[['UserID','MovieID']]\n",
        "nusers=max(np.unique(ratings['UserID']))\n",
        "nmovies=max(np.unique(ratings['MovieID']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "saxcPzXE4ZXP"
      },
      "outputs": [],
      "source": [
        "def define_model_NeuMF(num_users,num_items,num_layers,predictivefactors,lr):\n",
        "  #defining optimizers based on paper\n",
        "  adamopt=keras.optimizers.Adam()\n",
        "  sgdopt=keras.optimizers.SGD(learning_rate=lr)\n",
        "\n",
        "  #Inputs\n",
        "  Movies = keras.layers.Input(shape=(1,),dtype='int32',name='Movies_Input')\n",
        "  Users = keras.layers.Input(shape=(1,),dtype='int32',name='Users_Input')\n",
        "  \n",
        "  #Movies = keras.utils.to_categorical(Movies,num_classes=num_items+1)\n",
        "  #Users = keras.utils.to_categorical(Users,num_classes=num_users+1)\n",
        "  #Movies=tf.one_hot(Movies,depth=num_items+1)\n",
        "  #Users=tf.one_hot(Users,depth=num_users+1)\n",
        "  \n",
        "  #GMF\n",
        "  gmf_userembed=keras.layers.Embedding(num_users+1,2*predictivefactors,name='GMF_Embed_U')(Users)\n",
        "  gmf_movieembed=keras.layers.Embedding(num_items+1,2*predictivefactors,name='GMF_Embed_M')(Movies)\n",
        "  gmf_uservec=keras.layers.Flatten(name='GMF_Flatten_U')(gmf_userembed)\n",
        "  gmf_movievec=keras.layers.Flatten(name='GMF_Flatten_M')(gmf_movieembed)\n",
        "  gmf_vec=keras.layers.multiply([gmf_uservec,gmf_movievec],name='GMF_multiply')\n",
        "  gmf = keras.layers.Dense(1, activation=\"sigmoid\",kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01),name='GMF_Dense')(gmf_vec)\n",
        "  gmf_model = keras.Model(inputs=[Movies,Users],outputs=gmf)\n",
        "  gmf_model.compile(optimizer=adamopt,loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  \n",
        "  #MLP\n",
        "  mlp_userembed=keras.layers.Embedding(num_users+1,2*predictivefactors,name='MLP_Embed_U')(Users)\n",
        "  mlp_movieembed=keras.layers.Embedding(num_items+1,2*predictivefactors,name='MLP_Embed_M')(Movies)\n",
        "  mlp_uservec=keras.layers.Flatten(name='MLP_Flatten_U')(mlp_userembed)\n",
        "  mlp_movievec=keras.layers.Flatten(name='MLP_Flatten_M')(mlp_movieembed)\n",
        "  mlp_vec=keras.layers.concatenate([mlp_uservec,mlp_movievec],name='MLP_Concat')\n",
        "  mlp=  keras.layers.Dense((2**(num_layers-1))*predictivefactors, activation=\"relu\",kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01),name=('MLP_Dense_'+str(1)))(mlp_vec)\n",
        "  \n",
        "  for i in range(num_layers-1):\n",
        "    mlp = keras.layers.Dense((2**(num_layers-2-i))*predictivefactors, activation=\"relu\",kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01),name=('MLP_Dense_'+str(2+i)))(mlp)\n",
        "  \n",
        "  mlp_out=  keras.layers.Dense(1, activation=\"sigmoid\",kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01),name=('MLP_Dense_out'))(mlp)\n",
        "  mlp_model = keras.Model(inputs=[Movies,Users], outputs=mlp_out)\n",
        "  mlp_model.compile(optimizer=adamopt,loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "  #NeuMF\n",
        "  NeuMF_vec = keras.layers.concatenate([mlp, gmf_model.output],name='NeuMF_Concat')\n",
        "  NeuMF = keras.layers.Dense(1, activation=\"sigmoid\",kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01),name='NeuMF_Dense')(NeuMF_vec)\n",
        "  # our model will accept the inputs of the two branches and\n",
        "  # then output a single value\n",
        "  neumf_model = keras.Model(inputs=[gmf_model.input], outputs=NeuMF)\n",
        "  neumf_model.compile(optimizer=sgdopt,loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return gmf_model,mlp_model,neumf_model\n",
        "\n",
        "def define_model_NeuMF_pretrained(model1,model2,num_users,num_items,num_layers,predictivefactors,lr,flag=False,model3=None):\n",
        "  #defining a pretrained neumf based on trained GMF,MLP and optional neumf\n",
        "  adamopt=keras.optimizers.Adam()\n",
        "  sgdopt=keras.optimizers.SGD(learning_rate=lr)\n",
        "  Movies = keras.layers.Input(shape=(1,),dtype='int32',name='Movies_Input')\n",
        "  Users = keras.layers.Input(shape=(1,),dtype='int32',name='Users_Input')\n",
        "    \n",
        "  #GMF\n",
        "  gmf_userembed=keras.layers.Embedding(num_users+1,2*predictivefactors,name='GMF_Embed_U',weights=model1.layers[2].get_weights())(Users)\n",
        "  gmf_movieembed=keras.layers.Embedding(num_items+1,2*predictivefactors,name='GMF_Embed_M',weights=model1.layers[3].get_weights())(Movies)\n",
        "  gmf_uservec=keras.layers.Flatten(name='GMF_Flatten_U')(gmf_userembed)\n",
        "  gmf_movievec=keras.layers.Flatten(name='GMF_Flatten_M')(gmf_movieembed)\n",
        "  gmf_vec=keras.layers.multiply([gmf_uservec,gmf_movievec],name='GMF_multiply')\n",
        "  gmf = keras.layers.Dense(1, activation=\"sigmoid\",name='GMF_Dense',weights=model1.layers[7].get_weights())(gmf_vec)\n",
        "  \n",
        "  #MLP\n",
        "  mlp_userembed=keras.layers.Embedding(num_users+1,2*predictivefactors,name='MLP_Embed_U',weights=model2.layers[2].get_weights())(Users)\n",
        "  mlp_movieembed=keras.layers.Embedding(num_items+1,2*predictivefactors,name='MLP_Embed_M',weights=model2.layers[3].get_weights())(Movies)\n",
        "  mlp_uservec=keras.layers.Flatten(name='MLP_Flatten_U')(mlp_userembed)\n",
        "  mlp_movievec=keras.layers.Flatten(name='MLP_Flatten_M')(mlp_movieembed)\n",
        "  mlp_vec=keras.layers.concatenate([mlp_uservec,mlp_movievec],name='MLP_Concat')\n",
        "  mlp=keras.layers.Dense((2**(num_layers-1))*predictivefactors, activation=\"relu\",weights=model2.layers[7].get_weights(),name=('MLP_Dense_'+str(1)))(mlp_vec)\n",
        "  \n",
        "  for i in range(num_layers-1):\n",
        "    mlp = keras.layers.Dense((2**(num_layers-2-i))*predictivefactors, activation=\"relu\",weights=model2.layers[8+i].get_weights(),name=('MLP_Dense_'+str(2+i)))(mlp)\n",
        "\n",
        "  #NeuMF Layer\n",
        "  NeuMF_vec = keras.layers.concatenate([mlp, gmf],name='NeuMF_Concat')\n",
        "  if flag:\n",
        "    NeuMF = keras.layers.Dense(1, activation=\"sigmoid\",weights=model3.layers[-1].get_weights(),name='NeuMF_Dense')(NeuMF_vec)\n",
        "  else:\n",
        "    NeuMF = keras.layers.Dense(1, activation=\"sigmoid\",kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01),name='NeuMF_Dense')(NeuMF_vec)\n",
        "    \n",
        "  neumf_model = keras.Model(inputs=[Movies,Users], outputs=NeuMF)\n",
        "  neumf_model.compile(optimizer=sgdopt,loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return neumf_model\n",
        "\n",
        "  \n",
        "def summarize_diagnostics(history,a,counter):\n",
        "\t# plot loss\n",
        "  pyplot.subplot(211)\n",
        "  pyplot.title('Categorical Cross Entropy Loss')\n",
        "  pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "  pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "  # plot accuracy\n",
        "  pyplot.subplot(212)\n",
        "  pyplot.title('Classification Accuracy')\n",
        "  pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "  pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "  if a==1:\n",
        "    filename = '/content/drive/My Drive/NCF/gmf_'+str(counter)\n",
        "  elif a==2:\n",
        "    filename = '/content/drive/My Drive/NCF/mlp_'+str(counter)\n",
        "  elif a==3:\n",
        "    filename = '/content/drive/My Drive/NCF/neumf_'+str(counter)\n",
        "  elif a==4:\n",
        "    filename = '/content/drive/My Drive/NCF/tneumf_'+str(counter)\n",
        "  pyplot.savefig(filename + '_plot.png')\n",
        "  pyplot.close()\n",
        "  return np.nan\n",
        "\n",
        "def evaluate(df,predvec,k):\n",
        "  df['pred']=predvec\n",
        "  hitratio=hr(df,k)\n",
        "  ndcgscore=ndcg(df,k)\n",
        "  return hitratio,ndcgscore\n",
        "def hr(df,k):\n",
        "  hit=0.0\n",
        "  for i in range(len(np.unique(df['UserID']))):\n",
        "    #seprating each user predictions\n",
        "    _df=df[df['UserID']==i+1]\n",
        "    _df=_df.sort_values('pred', ascending=False)\n",
        "    if np.argwhere(_df['label'].values==1).squeeze()<k:\n",
        "      hit+=1\n",
        "  hr=hit/len(np.unique(df['UserID']))\n",
        "  return hr\n",
        "def ndcg(df,k):\n",
        "  ndcg=0\n",
        "  for i in range(len(np.unique(df['UserID']))):\n",
        "    _df=df[df['UserID']==i+1]\n",
        "    _df=_df.sort_values('pred', ascending=False)\n",
        "    loc=np.argwhere(_df['label'].values==1).squeeze()\n",
        "    if loc<=k:\n",
        "      ndcg+=(1/np.log2(loc+2))/(1/np.log2(2))    \n",
        "  ndcg=ndcg/len(np.unique(df['UserID']))\n",
        "  return ndcg\n",
        "\n",
        "def preprocess(df,negativeratio):\n",
        "  random.seed=1\n",
        "  #creating user item interaction matrix\n",
        "  pdf=pd.crosstab(df.iloc[:,0],df.iloc[:,1])\n",
        "  ndf=~pdf+2\n",
        "\n",
        "  #creating empty dataframes to store needed data\n",
        "  test=pd.DataFrame()\n",
        "  validation=pd.DataFrame()\n",
        "  train=pd.DataFrame()\n",
        "\n",
        "  for i in range(len(np.unique(df['UserID']))):\n",
        "    #seprating positive instances\n",
        "    temp=df[df['UserID']==i+1].iloc[-1,:]\n",
        "    temp['label']=1\n",
        "    test=test.append(temp,ignore_index=True)    \n",
        "    \n",
        "    temp=df[df['UserID']==i+1].iloc[-2,:]\n",
        "    temp['label']=1\n",
        "    validation=validation.append(temp,ignore_index=True)\n",
        "    \n",
        "    temp=df[df['UserID']==i+1].iloc[:-2,:]\n",
        "    temp['label']=1\n",
        "    train=train.append(temp,ignore_index=True)\n",
        "    \n",
        "    #adding negative instances\n",
        "    if len(np.flatnonzero(ndf.iloc[i,:]))>(negativeratio*len(np.flatnonzero(pdf.iloc[i,:]))+100):\n",
        "      ids=random.sample(list(np.flatnonzero(ndf.iloc[i,:])),(negativeratio*len(np.flatnonzero(pdf.iloc[i,:]))+100))\n",
        "    else:\n",
        "      ids=random.sample(list(np.flatnonzero(ndf.iloc[i,:])),len(list(np.flatnonzero(ndf.iloc[i,:]))))\n",
        "    \n",
        "    _a=pd.DataFrame(ids[:negativeratio], columns=['MovieID'])  \n",
        "    _a['UserID']=i+1\n",
        "    _a['label']=0\n",
        "    validation=validation.append(_a,ignore_index=True)\n",
        "    _a=pd.DataFrame(ids[negativeratio:negativeratio+100], columns=['MovieID'])  \n",
        "    _a['UserID']=i+1\n",
        "    _a['label']=0\n",
        "    test=test.append(_a,ignore_index=True)\n",
        "    _a=pd.DataFrame(ids[negativeratio+100:], columns=['MovieID'])  \n",
        "    _a['UserID']=i+1\n",
        "    _a['label']=0\n",
        "    train=train.append(_a,ignore_index=True)\n",
        "  return train.astype('int'),test.astype('int'),validation.astype('int')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "colab_type": "code",
        "id": "C1cz-bdeH2GS",
        "outputId": "12e01db8-7698-4659-fd9b-54bfed52b0fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  sort=sort,\n"
          ]
        }
      ],
      "source": [
        "train,test,validation=preprocess(ratings,ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1DB_jcJMr1oq"
      },
      "outputs": [],
      "source": [
        "counter=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "R4F6kIeJZ4w1"
      },
      "outputs": [],
      "source": [
        "nlayers=3\n",
        "pf=32\n",
        "lrr=.005\n",
        "bs=1024\n",
        "ratio=4\n",
        "\n",
        "gmf,mlp,neumf=define_model_NeuMF(nusers,nmovies,nlayers,pf,lrr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "FoEzctx1NgfV",
        "outputId": "25b1dc46-2b42-40f4-82d5-76bbc1c4a941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.46202, saving model to /content/drive/My Drive/NCF/gmf_model2.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.46202 to 0.35252, saving model to /content/drive/My Drive/NCF/gmf_model2.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.35252 to 0.30203, saving model to /content/drive/My Drive/NCF/gmf_model2.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.30203 to 0.28179, saving model to /content/drive/My Drive/NCF/gmf_model2.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.28179 to 0.27268, saving model to /content/drive/My Drive/NCF/gmf_model2.h5\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.27268 to 0.26985, saving model to /content/drive/My Drive/NCF/gmf_model2.h5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.26985\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.26985\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.26985\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.26985\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.26985\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.26985\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.26985\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.26985\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.26985\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.26985\n",
            "Epoch 00016: early stopping\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.36611, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.36611 to 0.36238, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.36238 to 0.31230, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.31230 to 0.29452, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.29452 to 0.28894, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.28894 to 0.28154, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.28154 to 0.27814, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.27814 to 0.27427, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.27427 to 0.27374, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.27374 to 0.27154, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.27154\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.27154 to 0.27002, saving model to /content/drive/My Drive/NCF/mlp_model2.h5\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.27002\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.27002\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.27002\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.27002\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.27002\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.27002\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.27002\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.27002\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.27403, saving model to /content/drive/My Drive/NCF/neumf_model2.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.27403\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.27403\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.27403\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.27403\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.27403\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.27403\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.27403\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.27403\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.27403\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.27403\n",
            "Epoch 00011: early stopping\n"
          ]
        }
      ],
      "source": [
        "counter+=1\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "mcgmf = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/NCF/gmf_model'+str(counter)+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "mcmlp = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/NCF/mlp_model'+str(counter)+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "mcneumf = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/NCF/neumf_model'+str(counter)+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "tmcneumf = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/NCF/t_neumf_model'+str(counter)+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "\n",
        "gmf_history = gmf.fit([train.MovieID,train.UserID],y=train.label,epochs=20,shuffle=True,batch_size=bs,verbose=0,\n",
        "                      validation_data=([validation.MovieID,validation.UserID],validation.label),callbacks=[es,mcgmf])\n",
        "summarize_diagnostics(gmf_history,1,counter)\n",
        "\n",
        "\n",
        "mlp_history = mlp.fit([train.MovieID,train.UserID],y=train.label,epochs=20,shuffle=True,batch_size=bs,verbose=0,\n",
        "                      validation_data=([validation.MovieID,validation.UserID],validation.label),callbacks=[es,mcmlp])\n",
        "summarize_diagnostics(mlp_history,2,counter)\n",
        "\n",
        "\n",
        "neumf_history = neumf.fit([train.MovieID,train.UserID],y=train.label,epochs=100,shuffle=True,batch_size=bs,verbose=0,\n",
        "                          validation_data=([validation.MovieID,validation.UserID],validation.label),callbacks=[es,mcneumf])\n",
        "summarize_diagnostics(neumf_history,3,counter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "colab_type": "code",
        "id": "szr5OH1ccUEL",
        "outputId": "a9e0a739-944b-4d3c-fea6-ea4bf32f0d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.27532, saving model to /content/drive/My Drive/NCF/t_neumf_model2.h5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.27532\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.27532\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.27532\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.27532\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.27532\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.27532\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.27532\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.27532\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.27532\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.27532\n",
            "Epoch 00011: early stopping\n"
          ]
        }
      ],
      "source": [
        "trained_neumf=define_model_NeuMF_pretrained(gmf,mlp,nusers,nmovies,nlayers,pf,lrr,flag=False,model3=None)\n",
        "t_neumf_history = trained_neumf.fit([train.MovieID,train.UserID],y=train.label,epochs=100,shuffle=True,batch_size=bs,verbose=0,\n",
        "                          validation_data=([validation.MovieID,validation.UserID],validation.label),callbacks=[es,tmcneumf])\n",
        "summarize_diagnostics(t_neumf_history,4,counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "8Pd8kr76Pszn",
        "outputId": "b155b35b-2330-4b37-9166-c978cd1b3cd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:\n",
            "gmf [0.91439791]\n",
            "mlp: [0.91055511]\n",
            "neumf: [0.91781023]\n",
            "tneumf: [0.91753725]\n"
          ]
        }
      ],
      "source": [
        "print('accuracy:')\n",
        "print('gmf',sum((gmf.predict([test.MovieID,test.UserID])>.5)==test.label.values.reshape(len(test.label.values),1))/len(test.label.values))\n",
        "print('mlp:',sum((mlp.predict([test.MovieID,test.UserID])>.5)==test.label.values.reshape(len(test.label.values),1))/len(test.label.values))\n",
        "print('neumf:',sum((neumf.predict([test.MovieID,test.UserID])>.5)==test.label.values.reshape(len(test.label.values),1))/len(test.label.values))\n",
        "print('tneumf:',sum((trained_neumf.predict([test.MovieID,test.UserID])>.5)==test.label.values.reshape(len(test.label.values),1))/len(test.label.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "colab_type": "code",
        "id": "mV5eirz6d90Y",
        "outputId": "b2fe9d40-c825-43f5-a5ac-a833a1493cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topk - k= 1\n",
            "gmf: (0.14846235418875928, 0.21403087576882543)\n",
            "mlp: (0.1569459172852598, 0.23522670325329798)\n",
            "neumf: (0.1633085896076352, 0.2389131093887318)\n",
            "tneumf: (0.1569459172852598, 0.2332195036130918)\n",
            "topk - k= 2\n",
            "gmf: (0.2523860021208908, 0.25591846855779676)\n",
            "mlp: (0.28101802757158006, 0.29090008607408285)\n",
            "neumf: (0.2831389183457052, 0.2898144879677352)\n",
            "tneumf: (0.27783669141039236, 0.2830604368050325)\n",
            "topk - k= 3\n",
            "gmf: (0.3361611876988335, 0.2938253130117649)\n",
            "mlp: (0.39236479321314954, 0.3260666767121018)\n",
            "neumf: (0.38494167550371156, 0.3322884221149529)\n",
            "tneumf: (0.37751855779427357, 0.32599107992157517)\n",
            "topk - k= 4\n",
            "gmf: (0.4241781548250265, 0.3131064179322566)\n",
            "mlp: (0.4740190880169671, 0.3494501443816342)\n",
            "neumf: (0.4835630965005302, 0.3569025986091975)\n",
            "tneumf: (0.47720042417815484, 0.3514257289656279)\n",
            "topk - k= 5\n",
            "gmf: (0.4740190880169671, 0.3350152374998765)\n",
            "mlp: (0.5344644750795334, 0.3664483664599599)\n",
            "neumf: (0.5471898197242842, 0.37238986761389414)\n",
            "tneumf: (0.542948038176034, 0.3680462127755463)\n",
            "topk - k= 6\n",
            "gmf: (0.5355249204665959, 0.34950799112306513)\n",
            "mlp: (0.5821845174973489, 0.38200156547021097)\n",
            "neumf: (0.5906680805938495, 0.3861756576457076)\n",
            "tneumf: (0.5896076352067868, 0.3793576302375471)\n",
            "topk - k= 7\n",
            "gmf: (0.5790031813361611, 0.3612166556909342)\n",
            "mlp: (0.6288441145281018, 0.3930411634913447)\n",
            "neumf: (0.6320254506892895, 0.39520805602663517)\n",
            "tneumf: (0.623541887592789, 0.3897281617119454)\n",
            "topk - k= 8\n",
            "gmf: (0.616118769883351, 0.3688780765773979)\n",
            "mlp: (0.6638388123011665, 0.40166026198861654)\n",
            "neumf: (0.6606574761399788, 0.40510405800498417)\n",
            "tneumf: (0.6564156945917285, 0.40090106717137175)\n",
            "topk - k= 9\n",
            "gmf: (0.6415694591728526, 0.3802199626577391)\n",
            "mlp: (0.6924708377518558, 0.4099367734526492)\n",
            "neumf: (0.6935312831389183, 0.4133805694690169)\n",
            "tneumf: (0.6935312831389183, 0.40825796625051197)\n",
            "topk - k= 10\n",
            "gmf: (0.6808059384941676, 0.3864318416170961)\n",
            "mlp: (0.721102863202545, 0.4143738298521899)\n",
            "neumf: (0.7221633085896076, 0.42107146722822086)\n",
            "tneumf: (0.71898197242842, 0.4165404715296547)\n"
          ]
        }
      ],
      "source": [
        "for tk in range(1,11):\n",
        "  print('topk - k=',tk)\n",
        "  print('gmf:',evaluate(test,gmf.predict([test.MovieID,test.UserID]),tk))\n",
        "  print('mlp:',evaluate(test,mlp.predict([test.MovieID,test.UserID]),tk))\n",
        "  print('neumf:',evaluate(test,neumf.predict([test.MovieID,test.UserID]),tk))\n",
        "  print('tneumf:',evaluate(test,trained_neumf.predict([test.MovieID,test.UserID]),tk))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "i6bx5XU0683K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NCF",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
